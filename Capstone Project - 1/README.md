# Introduction
## Prelude
This is a capstone project for the [Machine Learning Zoomcamp](https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp) by DTC.
Although I am almost done with another project, it was a supervised learning project, and with the Kitchenware competition being a thing, I thought maybe I should use this chance to actually work with a neural network, so here it is.

## Dataset
The data used was obtained through [kaggle](https://www.kaggle.com/competitions/kitchenware-classification) and is a part of a competition to build a machine learning model to classify kitchenware and utinsels


## Problem Description
the ultimate goal is the identification of objects in images and classifying them into one of 6 classes, these classes being:
- cup
- fork
- glass
- knife
- plate
- spoon <br>

we will do this by exploring the images, training different models, hypertuning the models' parameters, and seeing how well they will do with the predictions. 


## Project Layout
- `docker folder`: includes a dockerfile and entrypoint.sh that are automatically generated by BentoML. It is only there for completion's sake.
- `model folder`: includes the model that was saved from the notebook and used to build the service
- `train.py`: script to train a model
- `notebook.ipynb`: notebook where the different models are explored, and trained
- `get_bento.py`: code to load model and save it to the bento registry
- `bentofile.yaml`: yaml file with specifications that are used to build the service container
- `serve.py`: code for model serving service

## Environment
The project was developed on saturncloud, if you use the notebook on their server, you should not need to download anything. additionally the bentofile takes care of the requirements for the serving layer. all in all, you mainly need tensorflow, pillow and numpy if you really want to run the notebook locally, but the rest, it should take care of itself.

# The Model:
### EDA
The percentage and distribution of images were explored, as well as the content of different images.
In the notebook there is a cell that generates 9 random images per activation, you can use this to explore a wide range of images.

### Model Training
In the notebook, 4 models were trained, Xception, EfficientNet, DenseNet and Inception. The performance for EfficientNet and Xception were the best, with EfficientNet being slightly better at times. I ended up going with Xception, so naturally I explored its hyperparameters.
The hyperparameters that were explored were the learning rate, dense layer size and drop out rate.
After running everything, it seems like only optimizing the learning rate yields best result (0.94263 compared to 0.90490 public score in kaggle) so that is what we went with.
The model was exported and with the help of bento it was deployed.

# Project Rubric
EDA & Model Training already covered so we will proceed from there.
## Exporting Notebook to script
the script can be found in train.py
## Reproducability 
Assuming you use the notebook in saturn, and setup the data folder properly, everything should be running without a fault.
## Model Deployment
Deployed with BentoML
## Dependency & Environment
Bentofile provided
## Containerization 
Dockerfile provided
## Cloud deployment
Deployed to the cloud using Serverless Cloud Run Architecture
to reproduce simply follow these steps (I will start from the moment you save your model using get_bento.py)

1. In the directory with the bentofile and service, run this command, it will return a message saying something like `successfully built kitchenware-classifier-service:j4ayireatk3xmasc`
```
bentoml build
```
2. containerize the service
```
bentoml containerize kitchenware-classifier-service:j4ayireatk3xmasc
```
3. tag it and push it to a docker registry, in my case I pushed it to GCP container registry
```
docker tag kitchenware-classifier-service:j4ayireatk3xmasc eu.gcr.io/${PROJECT}/kitchenware-classifier-service:j4ayireatk3xmasc
docker push eu.gcr.io/${PROJECT}/kitchenware-classifier-service:j4ayireatk3xmasc
```
4. Finally, deploy the image as a service using gcp serverless cloud run easy deployments
```
gcloud run deploy kitchware-classification --image eu.gcr.io/${PROJECT}/kitchenware-classifier-service:j4ayireatk3xmasc --allow-unauthenticated --port 3000 --project $PROJECT --memory 1Gi
```
kindly note that --memory flag is cruical here, with a machine learning model, it will never boot up with the default 512mb memory.

this will provide you with a link that is accessible anywhere, which can be queried. this is illustrated in the video below, wherein some images were obtained from google, and used to test the model.

https://user-images.githubusercontent.com/107177143/208791837-0c0f8b8b-a8a0-471c-83f0-173f148dfa04.mp4


